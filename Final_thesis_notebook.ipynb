{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this thesis, 5000 sentences have been used. A lower amount, such as 1000, would give\n",
    "reasonable similair results'''\n",
    "\n",
    "number_of_sentences = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "import pickle\n",
    "import os\n",
    "from ursa import kernel\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import extract_embeddings\n",
    "import modeling\n",
    "from BERT import tokenization\n",
    "import jsonlines\n",
    "import tensorflow\n",
    "from skip_thoughts import configuration\n",
    "from skip_thoughts import encoder_manager\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load data from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retieve_forest_and_source(data_path, forest_folder, source_folder):\n",
    "    '''Loads the sentence trees and strings from their respective location. The trees in the forest\n",
    "    and the sentences in the source are alligned by index.'''\n",
    "    forest_folder_count = 0\n",
    "    source_folder_count = 0\n",
    "    for root, dirs, files in os.walk(os.path.join(data_path, forest_folder)):\n",
    "        forest_folder_count += len(dirs)\n",
    "    for root, dirs, files in os.walk(os.path.join(data_path, source_folder)):\n",
    "        source_folder_count += len(dirs)\n",
    "    source = []\n",
    "    forest = []\n",
    "    file_names = []\n",
    "    for i in range(forest_folder_count):\n",
    "        folder = str(i)\n",
    "        if len(folder) == 1:\n",
    "            folder = \"0\" + folder\n",
    "        tree_files = 0\n",
    "        source_files = 0\n",
    "        for root, dirs, files in os.walk(os.path.join(data_path, forest_folder, folder)):\n",
    "            tree_files += len(files)\n",
    "        for root, dirs, files in os.walk(os.path.join(data_path, source_folder, folder)):\n",
    "            source_files += len(files)\n",
    "        assert(tree_files == source_files)\n",
    "        if folder == \"00\": \n",
    "            for file in range(0, tree_files-1):\n",
    "                if(len(str(file)) == 1):\n",
    "                    file = os.path.join(folder, \"wsj_\" + folder + \"0\" + str(file))\n",
    "                    file_names.append(file)\n",
    "                else:\n",
    "                    file = os.path.join(folder,\"wsj_\" + folder + str(file))\n",
    "                    file_names.append(file)\n",
    "        else:\n",
    "            for file in range(0, tree_files-1):\n",
    "                if(len(str(file)) == 1):\n",
    "                    file = os.path.join(folder,\"wsj_\" + folder + \"0\" + str(file))\n",
    "                    file_names.append(file)\n",
    "                else:\n",
    "                    file = os.path.join(folder,\"wsj_\" + folder + str(file))\n",
    "                    file_names.append(file)\n",
    "    del file_names[0]\n",
    "\n",
    "    for i in file_names:\n",
    "        tree_file = open(os.path.join(data_path, forest_folder, i + '.tree'), 'r')\n",
    "        for sentence in tree_file:\n",
    "            forest.append(sentence)\n",
    "        tree_file.close()\n",
    "        source_file = open(os.path.join(data_path, source_folder, i + '.txt'), 'r')\n",
    "        #maybe change this to readlines\n",
    "        for sentence in source_file:\n",
    "            source.append(sentence)\n",
    "        source_file.close()\n",
    "    assert(len(source) == len(forest))\n",
    "    return(forest, source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest, source = retieve_forest_and_source('Penn Treebank\\eng_news_txt_tbnk-ptb_revised\\data', 'penntree', 'tokenized_source')\n",
    "\n",
    "forest = forest[0:number_of_sentences]\n",
    "source = source[0:number_of_sentences]\n",
    "\n",
    "\n",
    "forest_file = open('forest_file', 'wb')\n",
    "pickle.dump(forest, forest_file)\n",
    "forest_file.close()\n",
    "\n",
    "#retieve_forest_and_source function did not remove the indeces in the string itself\n",
    "for c, i in enumerate(source):\n",
    "    source[c] = i.split(\">\")[1]\n",
    "\n",
    "source_file = open('source_file', 'wb')\n",
    "pickle.dump(source, source_file)\n",
    "source_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source and forest from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_file = open('forest_file','rb')  \n",
    "source_file = open('source_file','rb') \n",
    "\n",
    "\n",
    "string_forest = pickle.load(forest_file)\n",
    "#converts the parsed sentenced that are stored as string to nltk tree objects.\n",
    "forest = [Tree.fromstring(tree) for tree in string_forest]\n",
    "source = pickle.load(source_file)\n",
    "forest_file.close()\n",
    "source_file.close()\n",
    "#check to see if forest and source are aligned.\n",
    "print(len(forest) == len(source))\n",
    "print(forest[-1])\n",
    "print(source[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax similarity \n",
    "\n",
    "Computes the syntax similarity over the sub-trees (ST) and the sub set-trees (SST). To use the SST, set alpha = 1, to use ST, set alpha = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_syntax_DSM(trees, alpha):\n",
    "    k = kernel.Kernel(alpha = alpha)\n",
    "    syntax_similarity_matrix = np.empty((len(trees),len(trees)))\n",
    "\n",
    "    for i in range(0, len(trees)):\n",
    "        for j in range(i, len(trees)):\n",
    "            #syntax_similarity_matrix[i,j] = float(k(trees[i], trees[j])) / np.sqrt(float(k(trees[i], trees[i])) * float(k(trees[j], trees[j])))\n",
    "            #syntax_similarity_matrix[i,j] = float(k.C(trees[i], trees[j])) / np.sqrt(float(k.C(trees[i], trees[i])) * float(k.C(trees[j], trees[j])))\n",
    "            syntax_similarity_matrix[i,j] = 1 - (k.pairwise(trees[i], trees[j], normalize = True))\n",
    "            syntax_similarity_matrix[j,i] = syntax_similarity_matrix[i,j]\n",
    "\n",
    "    return syntax_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = kernel.Kernel()\n",
    "syntax_DSM = abs(k.pairwise(forest, normalize = True) - 1)\n",
    "\n",
    "syntax_DSM_file = open('syntax_DSM_file', 'wb')\n",
    "\n",
    "pickle.dump(syntax_DSM, syntax_DSM_file)\n",
    "syntax_DSM_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = kernel.Kernel(alpha = 0.1)\n",
    "syntax_DSM_a01 = abs(k.pairwise(forest, normalize = True) - 1)\n",
    "\n",
    "syntax_DSM_file_a01 = open('syntax_DSM_file_a01', 'wb')\n",
    "\n",
    "pickle.dump(syntax_DSM_a01, syntax_DSM_file_a01)\n",
    "syntax_DSM_file_a01.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_len(text):\n",
    "    '''Iterates to a text file with sentences, uses the tokenizer of BERT and checks\n",
    "    sequence len. Max sequence len is necessary for BERT embeddings input.'''\n",
    "    #text = open(file, \"r\")\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=r\"uncased_L-12_H-768_A-12/vocab.txt \",\\\n",
    "                                          do_lower_case=True)\n",
    "    max_sequence_len = 0\n",
    "    for sentence in text:\n",
    "        if max_sequence_len < len(tokenizer.tokenize(sentence)) + 2:\n",
    "            max_sequence_len = len(tokenizer.tokenize(sentence)) + 2\n",
    "    return max_sequence_len\n",
    "\n",
    "def jsonl_to_np_matrix(file, token_index, n_sentences, sequence_len = 768):\n",
    "    '''Extracts the activations from the jsonl file and strores them in a np array \n",
    "    with shape(n_sentences, sequence_len. 768 is the output sequence len of BERT_uncased_L-12.)'''\n",
    "    sentence_embeddings = []\n",
    "    x = np.empty((n_sentences, sequence_len), dtype = None)\n",
    "    with jsonlines.open(file) as reader:\n",
    "        for i, obj in enumerate(reader.iter()):      \n",
    "            x[i] = np.array(obj[\"features\"][token_index][\"layers\"][0][\"values\"])\n",
    "    return(x)\n",
    "\n",
    "def cosine_dsim(x):\n",
    "    '''Computes the DISsimilarity matrix of an array with activations.\n",
    "    Since the matrix is symmetrical on the diagonal line, half of the \n",
    "    values can be copied to the other half.'''\n",
    "    m = np.zeros((x.shape[0], x.shape[0]))\n",
    "    for i, p in enumerate(x):\n",
    "        for j, q in enumerate(x):\n",
    "            m[i,j] = 1- p.dot(q) / (p.dot(p)**0.5 * q.dot(q)**0.5)\n",
    "            m[j,i] = m[i,j]\n",
    "    \n",
    "    return np.nan_to_num(m)\n",
    "\n",
    "source_file = open('source_file','rb')\n",
    "source = pickle.load(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting source to text file in order to be fed to BERT\n",
    "file = open(\"input.txt\", \"w\") \n",
    "with open(\"input.txt\", \"w\") as f:\n",
    "    for item in source:\n",
    "        f.write(item)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates empty folder if not created yet\n",
    "if not os.path.exists('BERT_base'):\n",
    "    os.makedirs('BERT_base')\n",
    "\n",
    "input_file = r\"input.txt\"\n",
    "bert_config_file = r\"uncased_L-12_H-768_A-12/bert_config.json\" \n",
    "max_seq_length = get_max_seq_len(r\"input.txt\")\n",
    "init_checkpoint = r\"uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "vocab_file = r\"uncased_L-12_H-768_A-12/vocab.txt \"\n",
    "lower_case = True\n",
    "batch_size = 32\n",
    "use_tpu = False\n",
    "use_one_hot = False\n",
    "use_checkpoint = True\n",
    "\n",
    "'''To go over all the layer individually is inefficient, however\n",
    "due to memory and cpu constraints, it was not feasible to run BERT once and \n",
    "store the activations in one or several files.'''\n",
    "for layer in range(0,12):\n",
    "    layer = [layer]\n",
    "    output_file = r\"BERT_base/embeddings_output_layer\" + str(layer) + \".jsonl\"\n",
    "    extract_embeddings.embed(input_file, output_file, layer, bert_config_file,\\\n",
    "                             max_seq_length, init_checkpoint, vocab_file, lower_case, batch_size,\\\n",
    "                             use_tpu, use_one_hot, use_checkpoint)\n",
    "\n",
    "for layer in range(0, 12):\n",
    "    embedding_first_layer = jsonl_to_np_matrix(r\"BERT_base/embeddings_output_layer\" + str([layer]) + \".jsonl\",\\\n",
    "                                              0, number_of_sentences)\n",
    "    dsim_matrix = cosine_dsim(embedding_first_layer)\n",
    "    dsim_file = open(r'BERT_base/dsim_matrix' +str([layer]), 'wb')\n",
    "    pickle.dump(dsim_matrix, dsim_file)\n",
    "    dsim_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('BERT_random'):\n",
    "    os.makedirs('BERT_random')\n",
    "\n",
    "input_file = r\"input.txt\"\n",
    "bert_config_file = r\"uncased_L-12_H-768_A-12/bert_config.json\" \n",
    "max_seq_length = get_max_seq_len(source)\n",
    "init_checkpoint = r\"uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "vocab_file = r\"uncased_L-12_H-768_A-12/vocab.txt \"\n",
    "lower_case = True\n",
    "batch_size = 32\n",
    "use_tpu = False\n",
    "use_one_hot = False\n",
    "use_checkpoint = False\n",
    "\n",
    "for layer in range(0,12):\n",
    "    layer = [layer]\n",
    "    output_file = r\"BERT_random/random_embeddings_output_layer\" + str(layer) + \".jsonl\"\n",
    "    extract_embeddings.embed(input_file, output_file, layer , bert_config_file,\\\n",
    "                             max_seq_length, init_checkpoint, vocab_file, lower_case, batch_size,\\\n",
    "                             use_tpu, use_one_hot, use_checkpoint)\n",
    "\n",
    "for layer in range(0,12):\n",
    "    embedding_first_layer = jsonl_to_np_matrix(r\"BERT_random/random_embeddings_output_layer\" + str([layer]) + \".jsonl\",\\\n",
    "                                              0, number_of_sentences)\n",
    "    dsim_matrix = cosine_dsim(embedding_first_layer)\n",
    "    dsim_file = open(r'BERT_random\\random_dsim_matrix' +str([layer]), 'wb')\n",
    "    pickle.dump(dsim_matrix, dsim_file)\n",
    "    dsim_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "data.extend([line.strip() for line in source])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_skt_encoder = encoder_manager.EncoderManager()\n",
    "bi_skt_encoder.load_model(configuration.model_config(bidirectional_encoder=True),\n",
    "                   vocabulary_file= r\"skipthought\\skip_thoughts_bi_2017_02_16\\vocab.txt\",\n",
    "                   embedding_matrix_file=r\"skipthought\\skip_thoughts_bi_2017_02_16\\embeddings.npy\",\n",
    "                   checkpoint_path=r\"skipthought\\skip_thoughts_bi_2017_02_16\\model.ckpt-500008\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_skt_embedding = bi_skt_encoder.encode(data)\n",
    "\n",
    "bi_skt_embedding_file = open('SKT\\bi_skt_embedding_file', 'wb')\n",
    "\n",
    "pickle.dump(bi_skt_embedding, bi_skt_embedding_file)\n",
    "bi_skt_embedding_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_skt_dsim_matrix = cosine_dsim(bi_skt_embedding)\n",
    "dsim_file = open(r'SKT\\bi_skt_dsim_matrix', 'wb')\n",
    "pickle.dump(bi_skt_dsim_matrix, dsim_file)\n",
    "dsim_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni-skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_skt_encoder = encoder_manager.EncoderManager()\n",
    "uni_skt_encoder.load_model(configuration.model_config(),\n",
    "                   vocabulary_file= r\"skipthought\\skip_thoughts_uni_2017_02_02\\vocab.txt\",\n",
    "                   embedding_matrix_file=r\"skipthought\\skip_thoughts_uni_2017_02_02\\embeddings.npy\",\n",
    "                   checkpoint_path=r\"skipthought\\skip_thoughts_uni_2017_02_02\\model.ckpt-501424\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_skt_embedding = uni_skt_encoder.encode(data)\n",
    "\n",
    "uni_skt_embedding_file = open(r'SKT\\uni_skt_embedding_file', 'wb')\n",
    "\n",
    "pickle.dump(uni_skt_embedding, uni_skt_embedding_file)\n",
    "uni_skt_embedding_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_skt_dsim_matrix = cosine_dsim(uni_skt_embedding)\n",
    "dsim_file = open(r'SKT\\uni_skt_dsim_matrix', 'wb')\n",
    "pickle.dump(uni_skt_dsim_matrix, dsim_file)\n",
    "dsim_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combi-skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r'SKT\\uni_skt_embedding_file', 'rb')\n",
    "uni_skt_embedding = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(r'SKT\\bi_skt_embedding_file', 'rb')\n",
    "bi_skt_embedding = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "combi_skt_embedding = np.hstack((uni_skt_embedding, bi_skt_embedding))\n",
    "\n",
    "combi_skt_embedding_file = open(r'SKT\\combi_skt_embedding_file', 'wb')\n",
    "\n",
    "pickle.dump(combi_skt_embedding, combi_skt_embedding_file)\n",
    "combi_skt_embedding_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi_skt_file = open(r'SKT\\combi_skt_embedding_file','rb') \n",
    "combi_skt_embedding = pickle.load(combi_skt_file)\n",
    "source_file.close()\n",
    "\n",
    "combi_skt_dsim_matrix = cosine_dsim(combi_skt_embedding)\n",
    "dsim_file = open(r'SKT\\combi_skt_dsim_matrix', 'wb')\n",
    "pickle.dump(combi_skt_dsim_matrix, dsim_file)\n",
    "dsim_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representational distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triu(matrix):\n",
    "    indices = np.triu_indices(matrix.shape[0], 1)\n",
    "    return matrix[indices]\n",
    "\n",
    "def correlate_matrices(matrix1, matrix2, measure):\n",
    "    if measure == \"Pearson\":\n",
    "        return scipy.stats.pearsonr(extract_triu(matrix1), extract_triu(matrix2))\n",
    "    elif measure == \"Spearman\":\n",
    "        return scipy.stats.spearmanr(extract_triu(matrix1), extract_triu(matrix2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between first and last layer of BERT_base\n",
    "first_layer_file = open(r\"BERT_base\\dsim_matrix[0]\" , 'rb')\n",
    "matrix1 = pickle.load(first_layer_file)\n",
    "first_layer_file.close()\n",
    "\n",
    "last_layer_file = open(r\"BERT_base\\dsim_matrix[11]\", 'rb')\n",
    "matrix2 = pickle.load(last_layer_file)\n",
    "last_layer_file.close()\n",
    "\n",
    "pearson, p_pearson = correlate_matrices(matrix1, matrix2, \"Pearson\")\n",
    "spearman, p_spearman = correlate_matrices(matrix1, matrix2, \"Spearman\")\n",
    "print(pearson, p_pearson)\n",
    "print(spearman, p_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between first and last layer of BERT_random\n",
    "first_layer_file = open(r\"BERT_random\\random_dsim_matrix[0]\" , 'rb')\n",
    "matrix1 = pickle.load(first_layer_file)\n",
    "first_layer_file.close()\n",
    "\n",
    "last_layer_file = open(r\"BERT_random\\random_dsim_matrix[11]\", 'rb')\n",
    "matrix2 = pickle.load(last_layer_file)\n",
    "last_layer_file.close()\n",
    "\n",
    "pearson_r, p_r_pearson = correlate_matrices(matrix1, matrix2, \"Pearson\")\n",
    "spearman_r, p_r_spearman = correlate_matrices(matrix1, matrix2, \"Spearman\")\n",
    "print(pearson_r, p_r_pearson)\n",
    "print(spearman_r, p_r_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_matrix(pickle_names1, pickle_names2, measure, symmetrical = True):\n",
    "    '''Creates a correlation matrix from data stored in pickles. If symmetrical = True,\n",
    "    pickle_names1 and pickle_names2 should be identical'''\n",
    "    correlations = np.zeros((len(pickle_names1), len(pickle_names2)))\n",
    "    p = np.zeros((len(pickle_names1), len(pickle_names2)))\n",
    "    for i in range(len(pickle_names1)):\n",
    "        print(i)\n",
    "        matrix1_file = open(pickle_names1[i], 'rb')\n",
    "        matrix1 = pickle.load(matrix1_file)\n",
    "        matrix1_file.close()\n",
    "        if symmetrical:\n",
    "            start = i\n",
    "        else:\n",
    "            start = 0\n",
    "        for j in range(start, len(pickle_names2)):\n",
    "            matrix2_file = open(pickle_names2[j], 'rb')\n",
    "            matrix2 = pickle.load(matrix2_file)\n",
    "            matrix2_file.close()\n",
    "            correlations[i,j], p[i,j] = correlate_matrices(matrix1, matrix2, measure)\n",
    "            if symmetrical:\n",
    "                correlations[j,i], p[j,i] = correlations[i,j], p[i,j]\n",
    "                \n",
    "    return correlations, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlating between langauge models and tree kernel\n",
    "embeddings_pickles = []\n",
    "for layer in range(12):\n",
    "    embeddings_pickles.append(r\"BERT_base\\dsim_matrix\" + str([layer]))\n",
    "for layer in range(12):\n",
    "    embeddings_pickles.append(r\"BERT_random\\random_dsim_matrix\" + str([layer]))\n",
    "embeddings_pickles.append(r\"SKT\\uni_skt_dsim_matrix\")\n",
    "embeddings_pickles.append(r\"SKT\\bi_skt_dsim_matrix\")\n",
    "embeddings_pickles.append(r\"SKT\\combi_skt_dsim_matrix\")\n",
    "syntax_pickles = [\"syntax_dsm_file\", \"syntax_dsm_file_a01\"]\n",
    "\n",
    "pearson_correlations, pearson_p = create_correlation_matrix(embeddings_pickles, syntax_pickles, \"Pearson\", symmetrical = False)\n",
    "spearman_correlations, spearman_p = create_correlation_matrix(embeddings_pickles, syntax_pickles, \"Spearman\", symmetrical = False)\n",
    "\n",
    "pearson_file = open('pearson_file', 'wb')\n",
    "pickle.dump([pearson_correlations, pearson_p], pearson_file)\n",
    "pearson_file.close()\n",
    "\n",
    "spearman_file = open('spearman_file', 'wb')\n",
    "pickle.dump([spearman_correlations, spearman_p], spearman_file)\n",
    "spearman_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting correlation matrices to nice pandas dataframes\n",
    "rows = [\"BERT_base layer \" + str(i) for i in range(12)] + [\"BERT_random layer \" + str(i) for i in range(12)] + [\"Uni-skip\", \"Bi-skip\", \"Combi-skip\"]\n",
    "\n",
    "pearson_file = open('pearson_file', 'rb')\n",
    "pearson_correlations, pearson_p = pickle.load(pearson_file)\n",
    "pearson_file.close()\n",
    "\n",
    "pearson_correlations_df = pd.DataFrame(pearson_correlations)\n",
    "pearson_correlations_df.columns = [\"Pear; a = 1\", \"Pear; a = 0.1\"]\n",
    "pearson_correlations_df.index = rows\n",
    "\n",
    "pearson_p_df = pd.DataFrame(pearson_p)\n",
    "pearson_p_df.columns = [\"Pear p; a = 1\", \"Pear p; a = 0.1\"]\n",
    "pearson_p_df.index = rows\n",
    "\n",
    "spearman_file = open('spearman_file', 'rb')\n",
    "spearman_correlations, spearman_p = pickle.load(spearman_file)\n",
    "spearman_file.close()\n",
    "\n",
    "spearman_correlations_df = pd.DataFrame(spearman_correlations)\n",
    "spearman_correlations_df.columns = [\"Spear; a = 1\", \"Spear; a = 0.1\"]\n",
    "spearman_correlations_df.index = rows\n",
    "\n",
    "spearman_p_df = pd.DataFrame(spearman_p)\n",
    "spearman_p_df.columns = [\"Spear p; a = 1\", \"Spear p; a = 0.1\"]\n",
    "spearman_p_df.index = rows\n",
    "\n",
    "df = pd.concat([pearson_correlations_df, pearson_p_df, spearman_correlations_df, spearman_p_df], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_plot(reference_RDM_pickle, embeddings_RDMs_pickle, sample_size, plot_name, sub_plot_names, file_name):\n",
    "    '''Accepts reference representations (in this case syntax) and embedding representations.\n",
    "    Plots the reference dissimilarity on the y-ax and embeddings dissimilarity on the x-ax and stores the plot.'''\n",
    "    f, axarr = plt.subplots(1, len(embeddings_RDMs_pickle), sharey=True, figsize=(15,3))\n",
    "    f.suptitle(plot_name, y=1.15)\n",
    "    \n",
    "    file = open(reference_RDM_pickle, 'rb')\n",
    "    syntax_matrix = pickle.load(file)\n",
    "    file.close()\n",
    "    syntax_triangle = extract_triu(syntax_matrix)\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(range(len(syntax_triangle)), size=sample_size, replace=False)\n",
    "    syntax_sample = syntax_triangle[sample_indices]\n",
    "    for c, i in enumerate(embeddings_RDMs_pickle):\n",
    "        file = open(i, 'rb')\n",
    "        embedding_matrix = pickle.load(file)\n",
    "        file.close()\n",
    "        axarr[c].scatter(syntax_sample, extract_triu(embedding_matrix)[sample_indices], alpha = 0.5)\n",
    "        axarr[c].set_xlim(0, 1) \n",
    "        axarr[c].set_ylim(0, 1) \n",
    "        axarr[c].plot((0, 1), \"r--\")\n",
    "        axarr[c].set_title(sub_plot_names[c], y=1.15)\n",
    "    \n",
    "    plt.savefig(\"images/\" + file_name + \".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def pickle_to_matshow(pickle_names, plot_name, sub_plot_names, file_name):\n",
    "    \"Displays the dissimilarity matrices from pickles in color matrices and stores the plots.\"\n",
    "    f, axarr = plt.subplots(1, len(pickle_names), sharey=True, figsize=(15,3))\n",
    "    \n",
    "    f.suptitle(plot_name, y=1.15)\n",
    "    for c, i in enumerate(pickle_names):\n",
    "        file = open(i, 'rb')\n",
    "        matrix = pickle.load(file)\n",
    "        file.close()\n",
    "        axarr[c].matshow(matrix)\n",
    "        axarr[c].set_title(sub_plot_names[c], y=1.15)\n",
    "    plt.savefig(\"images/\" + file_name + \".pdf\")\n",
    "    #plt.show()\n",
    "    plt.imshow(matrix)\n",
    "    plt.colorbar(extend='both')\n",
    "\n",
    "\n",
    "    \n",
    "def summarize(pickle_names):\n",
    "    '''Prints and stores the descriptives from the upper triangle of a matrix,\n",
    "    excluding the diagonal.'''\n",
    "    m = []\n",
    "    std = []\n",
    "    for c, i in enumerate(pickle_names):\n",
    "        file = open(i, 'rb')\n",
    "        matrix = pickle.load(file)\n",
    "        file.close()\n",
    "        triangle = extract_triu(matrix)\n",
    "        print(pickle_names[c])\n",
    "        print(\"Mean:\", np.mean(triangle))\n",
    "        print(\"Std:\", np.std(triangle))\n",
    "        print()\n",
    "        m.append(np.mean(triangle))\n",
    "        std.append(np.std(triangle))\n",
    "    return(m, std)\n",
    "\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_base_pear = list(df.iloc[0:12, [1]][\"Pear; a = 0.1\"])\n",
    "BERT_base_spear = list(df.iloc[0:12, [5]][\"Spear; a = 0.1\"])\n",
    "\n",
    "f = plt.figure()\n",
    "\n",
    "plt.plot(BERT_base_pear, color = \"red\", marker = \"x\", label = \"Pearson\")\n",
    "plt.plot(BERT_base_spear, color = \"blue\", marker = \"x\", label = \"Spearman\")\n",
    "plt.xlabel(\"Encoder layer\")\n",
    "plt.ylabel(\"Representational similarity\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(range(12))\n",
    "plt.legend()\n",
    "plt.title(\"Syntactic representation similarity of BERT base\")\n",
    "f.savefig('images/syn_rep.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 1\n",
    "pickle_to_matshow(['syntax_DSM_file', 'syntax_DSM_file_a01'], \"Syntax RDM's\", [\"a = 1\", \"a = 0.1\"], \"syntax_RDM_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_mean, syntax_std = summarize(['syntax_DSM_file', 'syntax_DSM_file_a01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_all_pickles = [r'BERT_base/dsim_matrix' +str([layer]) for layer in range(12)]\n",
    "bert_pickles_sample = [bert_all_pickles[i] for i in [0, 4, 8, 11]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_titles = ['Layer ' +str(layer) for layer in [0, 4, 8, 11]]\n",
    "\n",
    "pickle_to_matshow(bert_pickles_sample, \"BERT base RDM's\", bert_titles, \"BERT_base_RDM_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_plot(\"syntax_DSM_file_a01\", bert_pickles_sample, int(number_of_sentences/4), \"BERT base dissimilarity scatter\", bert_titles, \"BERT_base_RDM_scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_base_mean, BERT_base_std = summarize(bert_all_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,12)\n",
    "plt.figure()\n",
    "plt.errorbar(x, BERT_base_mean, BERT_base_std, linestyle='None', marker='^')\n",
    "plt.title(\"Dissimilarity BERT base\")\n",
    "plt.xticks(np.arange(12))\n",
    "plt.xlabel(\"Encoder layer\")\n",
    "plt.ylabel(\"Mean dissimilarity\")\n",
    "plt.savefig(\"images/base.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_r_all_pickles = [r'BERT_random/random_dsim_matrix' +str([layer]) for layer in range(12)]\n",
    "\n",
    "bert_r_pickles_sample = [bert_r_all_pickles[i] for i in [0, 4, 8, 11]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_r_titles = ['Layer ' +str(layer) for layer in [0, 4, 8, 11]]\n",
    "\n",
    "pickle_to_matshow(bert_r_pickles_sample, \"BERT random RDM's\", bert_r_titles, \"BERT_random_RDM_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_plot(\"syntax_DSM_file_a01\", bert_r_pickles_sample, int(number_of_sentences/4), \"BERT random dissimilarity scatter\", bert_r_titles, \"BERT_random_RDM_scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_r_mean, BERT_r_std = summarize(bert_r_all_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,12)\n",
    "plt.figure()\n",
    "plt.errorbar(x, BERT_r_mean, BERT_r_std, linestyle='None', marker='^')\n",
    "plt.title(\"Dissimilarity BERT random\")\n",
    "plt.xticks(np.arange(12))\n",
    "plt.xlabel(\"Encoder layer\")\n",
    "plt.ylabel(\"Mean dissimilarity\")\n",
    "plt.savefig(\"images/random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skt_pickles = [r\"SKT\\uni_skt_dsim_matrix\", r\"SKT\\bi_skt_dsim_matrix\", r\"SKT\\combi_skt_dsim_matrix\"]\n",
    "skt_titles = [\"Uni-skip\", \"Bi-skip\", \"Combi-skip\"]\n",
    "\n",
    "pickle_to_matshow(skt_pickles, \"Skip-Thought models\", skt_titles, \"SKT_RDM_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_plot(r\"syntax_DSM_file_a01\", skt_pickles, int(number_of_sentences/4), \"SKT dissimilarity scatter\", skt_titles, \"SKT_RDM_scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKT_base_mean, SKT_base_std = summarize(skt_pickles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, syntactical similair sentences are located for qualitative analyses. A lot of sentences in the Penn Treebank are almost identical with only one word or numeric value different. Therefore, the code below is been run untill the author found a pair of similair sentences that have a low syntactical dissimilarity, but seem to differ in semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similair(m, source):\n",
    "    x = np.unravel_index(m.argmin(), m.shape)\n",
    "    \n",
    "    if source[x[0]] == source[x[1]]:\n",
    "        m[x[0], x[1]] += 1\n",
    "        return(find_similair(m, source))\n",
    "    else:\n",
    "        m[x[0], x[1]] += 1\n",
    "        return m, m[x[0], x[1]]-1, source[x[0]], source[x[1]], x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"syntax_DSM_file_a01\", 'rb')\n",
    "matrix = pickle.load(file)\n",
    "file.close()\n",
    "for i in range(number_of_sentences):\n",
    "        matrix[i,i] +=1\n",
    "\n",
    "source_file = open('source_file','rb') \n",
    "source = pickle.load(source_file)\n",
    "source_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this till you find a satisfactory pair of sentences\n",
    "matrix, value, sen1, sen2, position = find_similair(matrix, source)\n",
    "print(sen1, sen2, value, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pickles = ['BERT_base/dsim_matrix' +str([layer]) for layer in range(12)]\n",
    "\n",
    "for i in bert_pickles:\n",
    "    file = open(i, 'rb')\n",
    "    matrix = pickle.load(file)\n",
    "    file.close()\n",
    "    print(i)\n",
    "    print(matrix[position])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[3464]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dsim = []\n",
    "for layer in range(12):\n",
    "    embeddings_dsim.append(r\"BERT_base\\dsim_matrix\" + str([layer]))\n",
    "for layer in range(12):\n",
    "    embeddings_dsim.append(r\"BERT_random\\random_dsim_matrix\" + str([layer]))\n",
    "embeddings_dsim.append(r\"SKT\\uni_skt_dsim_matrix\")\n",
    "embeddings_dsim.append(r\"SKT\\bi_skt_dsim_matrix\")\n",
    "embeddings_dsim.append(r\"SKT\\combi_skt_dsim_matrix\")\n",
    "embeddings_dsim.append(r\"syntax_dsm_file\")\n",
    "embeddings_dsim.append(r\"syntax_dsm_file_a01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a symmetrical correlation matrix of all the RDM's with respect to the diagonal.\n",
    "\n",
    "pear_corr_matrix, p_pear_corr_matrix,  = create_correlation_matrix(embeddings_pickles, embeddings_pickles, \"Pearson\")\n",
    "spear_corr_matrix, p_spear_corr_matrix = create_correlation_matrix(embeddings_pickles, embeddings_pickles, \"Spearman\")\n",
    "\n",
    "file = open('pearson_correlation_matrix', 'wb')\n",
    "pickle.dump(pear_corr_matrix, file)\n",
    "file.close()\n",
    "\n",
    "file = open('spearman_correlation_matrix', 'wb')\n",
    "pickle.dump(spear_corr_matrix, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pear_corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(spear_corr_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
